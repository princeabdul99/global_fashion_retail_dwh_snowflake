# Designing a Scalable Data Infrastructure for Global Fashion Retail
Led the design and implementation of a scalable, cloud-native data infrastructure to support centralized analytics for a global fashion retailer handling over 
- **4+ million transactions records**
- **35 Stores** accross 7 countries: 
ğŸ‡ºğŸ‡¸ United States | ğŸ‡¨ğŸ‡³ China | ğŸ‡©ğŸ‡ª Germany | ğŸ‡¬ğŸ‡§ United Kingdom | ğŸ‡«ğŸ‡· France | ğŸ‡ªğŸ‡¸ Spain | ğŸ‡µğŸ‡¹ Portugal

#### Challenge
The companyâ€™s existing data management process was heavily manual, fragmented, and slowâ€”leading to delayed insights, inconsistent reporting, and reactive decision-making.

#### Solution
Architected and implemented an automated data pipeline to extract raw data from **AWS S3**, ingest into **Snowflake**, and transform it into optimized reporting models. This enabled **real-time analytics**, reduced data latency, and ensured high data quality across countries.


#### Specifications
- **Data Sources**: Import data to cloud storage service system. e,g; AWS, Web Api (https://open.er-api.com/v6/latest/usd).
- **ELT/ETL development**: Extracting data from source systems and ingesting it into Snowflake, Performing data transformations and Presenting data to downstream consumers for analytics, reporting etc
- **Cost Control**: Monitor warehouse consumption and find use findings to strike a good balance between improving performance and controlling cost.
- **Optimization**: Optimize query performance by micro-partition pruning, utlize data caching and reduce data spiling.
- **Data Governance and access control**: Snowflake role-based access control (RBAC) model, where access privileges are assigned to roles that are granted to users.
- **CI/CD**: organize the data pipeline code in a Git repository, integrate it with Snowflake, and implement continuous integration and delivery.
- **Augment data with LLM**: Use Cortex LLM to interpret unstructured data.
- **Best Practices**: Design and implement data engineering solutions following best practices.


#### Team & Collaboration
Worked closely with:
- Data analysts from each branch country to align data outputs with local reporting needs.
- Project managers to manage security, access, and timelines.
- Cross-functional dev team to ensure smooth implementation and continuous delivery using GitHub Actions.

---
### Team
The project also simulate team of Data Engineer and Analyst to design and develop the scalable
data infrasture.

* ğŸ¤µ gfr_pm - the project manager user
* ğŸ§” gfr_dev_lead - the data enginer team lead
* ğŸ‘©â€ğŸ¦± gfr_ba_lead - the business analyst team lead
* ğŸ‘±â€â™€ï¸ gfr_ba_us - the analyst for United State region
* ğŸ‘¨â€ğŸ¦± gfr_ba_cn - the analyst for China region
* ğŸ‘±â€â™‚ï¸ gfr_ba_de - the analyst for Germany region
* ğŸ‘¨â€ğŸ¦² gfr_ba_gb - the analyst for United Kingdom region
* ğŸ‘¨â€ğŸ¦± gfr_ba_fr - the analyst for France region
* ğŸ‘©â€ğŸ¦± gfr_ba_es - the analyst for Spain region
* ğŸ‘¨â€ğŸ¦± gfr_ba_pt - the analyst for Portugal region






## ğŸ“‚ Repository Structure
The project structure uses a folder-by-data structure instead of folder-by-schema
```
global_fashion_retail_dwh_snowflake/
â”‚
â”œâ”€â”€ datasets/                             # Raw datasets used for the project (ERP, CRM data)
â”‚
â”œâ”€â”€ docs/                                 # Project documentation and architecture details
â”‚   â”œâ”€â”€ data_architecture.png             # Shows the project's architecture
â”‚   â”œâ”€â”€ data_catalog.md                   # Catalog of datasets, including field descriptions and metadata
â”‚   â”œâ”€â”€ data_flow.drawio                  # Draw.io file for the data flow diagram
â”‚   â”œâ”€â”€ data_models.drawio                # Draw.io file for data models (star schema)
â”‚   â”œâ”€â”€ naming-conventions.md             # Consistent naming guidelines for tables, columns, and files
â”‚
â”œâ”€â”€ csv_file_splitter/                    # Python script to split large csv file size into chunks
â”‚
â”œâ”€â”€ scripts/                              # SQL scripts for ETL and transformations
â”‚   â”œâ”€â”€ snowflake_objects/
â”‚       â”œâ”€â”€ /orchestration/
â”‚           â”œâ”€â”€ stores/                   # Tasks for ELT on stores data 
â”‚           â”œâ”€â”€ transitions/              # Tasks for ELT on transactions data 
â”‚           â”œâ”€â”€ .../              
â”‚           â”œâ”€â”€ pipeline_end_task.sql     # End task and send email notification   
â”‚           â”œâ”€â”€ pipeline_log.sql          # Tasks log
â”‚           â”œâ”€â”€ pipeline_start_task.sql   # Start task and send email notification
|
â”‚       â”œâ”€â”€ /stores/
â”‚           â”œâ”€â”€ ext/                      # Scripts for extracting and loading stores data using EXT schema
â”‚           â”œâ”€â”€ stg/                      # Scripts for cleaning and transforming stores data using STG schema
â”‚           â”œâ”€â”€ dwh/                      # Scripts that normalizes the stores data from the STG using DWH schema   
â”‚           â”œâ”€â”€ rpt/                      # Scripts for creating analytical reports using RPT schema
â”‚  
â”‚       â”œâ”€â”€ /transactions/
â”‚           â”œâ”€â”€ ext/                      # Scripts for extracting and loading stores data using EXT schema
â”‚           â”œâ”€â”€ stg/                      # Scripts for cleaning and transforming stores data using STG schema
â”‚           â”œâ”€â”€ dwh/                      # Scripts that normalizes the stores data from the STG using DWH schema   
â”‚           â”œâ”€â”€ rpt/                      # Scripts for creating analytical reports using RPT schema
â”‚       â”œâ”€â”€ /...
â”‚
â”‚   â”œâ”€â”€ deployment/
â”‚       â”œâ”€â”€ deploy_objects.sql            # Excute continuous integration with github
â”‚       â”œâ”€â”€ resume_tasks.sql              # Resume orchestration pipeline task
â”‚       â”œâ”€â”€ suspend_tasks.sql             # Suspend orchestration pipeline tasks
â”‚
â”‚   â”œâ”€â”€ rbac.sql                          # Script for Granting Privileges and Role access control
â”‚   â”œâ”€â”€ setup.sql                         # Script for creating project team roles, users and assign role privileges.
â”‚   â”œâ”€â”€ ddl.sql                           # Script for creating virtual warehouses, databases and schemas . 
â”‚
â”œâ”€â”€ tests/                                # Test scripts and quality files
â”‚
â”œâ”€â”€ README.md                             # Project overview and instructions
â”œâ”€â”€ LICENSE                               # License information for the repository
â”œâ”€â”€ .gitignore                            # Files and directories to be ignored by Git
â””â”€â”€ requirements.txt                      # Dependencies and requirements for the project
```
---
## Running Python Code
```
- **Install Environment**: In Terminal:
    cd csv_file_splitter
    run python -m venv venv
- **Activate Envr (windows)**: run venv\Scripts\Activate 
- **Run code**: python split_by_date.py
```
